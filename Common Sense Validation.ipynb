{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "i170160_D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgPZM4-0e-aK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986ec553-3297-441d-c3d9-0699fece695b"
      },
      "source": [
        "!pip install transformers==3.0.1 \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch, math\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import BertTokenizerFast\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")\n",
        "from transformers import *\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "config = BertConfig.from_pretrained('bert-large-uncased', num_labels=2)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.1 in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.1) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.1) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.1) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.1) (0.1.94)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.1) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.1) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.1) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.1) (0.8.0rc4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.1) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.1) (0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.1) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.1) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.1) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.1) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTVeBKRQff8H"
      },
      "source": [
        "class BertTwoSentences(BertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(BertTwoSentences, self).__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.pre_linear = nn.Linear(config.hidden_size, 300)\n",
        "        self.activation = nn.SELU()\n",
        "        self.reduce_fuse_linear = nn.Linear(600, 300)\n",
        "        self.cos = nn.CosineSimilarity()\n",
        "        self.rank_margin = nn.MarginRankingLoss(margin=0.4)\n",
        "        self.init_weights()\n",
        "    def forward(self, sentence1, sentence2, labels=None):\n",
        "        _, sentence1 = self.bert(sentence1)\n",
        "        sentence1 = self.activation(sentence1)\n",
        "        sentence1 = self.pre_linear(sentence1)\n",
        "        ori = sentence2\n",
        "        _, sentence2 = self.bert(sentence2)\n",
        "        sentence2 = self.activation(sentence2)\n",
        "        sentence2 = self.pre_linear(sentence2)\n",
        "        sent_concat = torch.cat((sentence1 + sentence2, sentence1 * sentence2), dim=1)\n",
        "        sent_concat = self.reduce_fuse_linear(sent_concat)\n",
        "        cos_sim1 = self.cos(sent_concat, sentence1)\n",
        "        cos_sim2 = self.cos(sent_concat, sentence2)\n",
        "        outputs = (cos_sim1, cos_sim2), None, None\n",
        "        if labels is not None:\n",
        "            labels[labels==0] = -1\n",
        "            loss_rank = self.rank_margin(cos_sim1, cos_sim2, labels)\n",
        "            outputs = (loss_rank,) + outputs\n",
        "        return outputs"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmVTQDJ2fvjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "134dd989-f3e6-4aa6-c4fc-118668aac216"
      },
      "source": [
        "import torch.nn as nn\n",
        "model = BertTwoSentences.from_pretrained('bert-large-uncased', config=config, num_labels=2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertTwoSentences: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertTwoSentences from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertTwoSentences from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertTwoSentences were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['pre_linear.weight', 'pre_linear.bias', 'reduce_fuse_linear.weight', 'reduce_fuse_linear.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STUiOmXFf0Y-"
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0wnf-tMgmW5"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8 )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "s-V26eOQfZwB",
        "outputId": "01685171-a308-4428-fa15-68e387b9738d"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload() #upload all 6 datasets here"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5a5f54b7-aaec-4a80-80c8-c7e611679283\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5a5f54b7-aaec-4a80-80c8-c7e611679283\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving subtaskA__dev_gold_answers.csv to subtaskA__dev_gold_answers (1).csv\n",
            "Saving subtaskA_answers_all.csv to subtaskA_answers_all (1).csv\n",
            "Saving subtaskA_data_all.csv to subtaskA_data_all (1).csv\n",
            "Saving subtaskA_dev_data.csv to subtaskA_dev_data (1).csv\n",
            "Saving subtaskA_gold_answers.csv to subtaskA_gold_answers (1).csv\n",
            "Saving subtaskA_test_data.csv to subtaskA_test_data (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i_WNJgojpUz"
      },
      "source": [
        "train_text = pd.read_csv(\"subtaskA_data_all.csv\")\n",
        "train_lab = pd.read_csv(\"subtaskA_answers_all.csv\",header=None)\n",
        "train_lab = train_lab.drop([0], axis=1)\n",
        "train_labels = train_lab.rename(columns={1: 'Invalid'})\n",
        "test_text = pd.read_csv(\"subtaskA_test_data.csv\")\n",
        "test_lab = pd.read_csv(\"subtaskA_gold_answers.csv\",header=None)\n",
        "test_lab = test_lab.drop([0], axis=1)\n",
        "test_labels = test_lab.rename(columns={1: 'Invalid'})\n",
        "val_text = pd.read_csv(\"subtaskA_dev_data.csv\")\n",
        "val_lab = pd.read_csv(\"subtaskA__dev_gold_answers.csv\",header=None)\n",
        "val_lab = val_lab.drop([0], axis=1)\n",
        "val_labels = val_lab.rename(columns={1: 'Invalid'})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR7-xZZ9faaZ"
      },
      "source": [
        "train_sen0, train_sen1 = train_text['sent0'], train_text['sent1']\n",
        "val_sen0, val_sen1 = val_text['sent0'], val_text['sent1']\n",
        "test_sen0, test_sen1 = test_text['sent0'], test_text['sent1']\n",
        "train_labels, val_labels, test_labels = train_labels['Invalid'], val_labels['Invalid'], test_labels['Invalid']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7IGdQJ7nZ81"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "total_tk_size = 20\n",
        "tokens_train0 = tokenizer.batch_encode_plus(train_sen0.tolist(), max_length = total_tk_size, pad_to_max_length=True, \n",
        "                                            truncation=True, return_token_type_ids=False)\n",
        "tokens_train1 = tokenizer.batch_encode_plus(train_sen1.tolist(), max_length = total_tk_size, pad_to_max_length=True,\n",
        "                                            truncation=True, return_token_type_ids=False)\n",
        "tokens_val0 = tokenizer.batch_encode_plus(val_sen0.tolist(), max_length = total_tk_size, pad_to_max_length=True, \n",
        "                                          truncation=True, return_token_type_ids=False)\n",
        "tokens_val1 = tokenizer.batch_encode_plus(val_sen1.tolist(), max_length = total_tk_size, pad_to_max_length=True,\n",
        "                                          truncation=True, return_token_type_ids=False)\n",
        "tokens_test0 = tokenizer.batch_encode_plus(test_sen0.tolist(), max_length = total_tk_size, pad_to_max_length=True,\n",
        "                                           truncation=True, return_token_type_ids=False)\n",
        "tokens_test1 = tokenizer.batch_encode_plus(test_sen1.tolist(), max_length = total_tk_size, pad_to_max_length=True, \n",
        "                                           truncation=True, return_token_type_ids=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ly1zM1noMI"
      },
      "source": [
        "# for train set 0 \n",
        "train_seq0 = torch.tensor(tokens_train0['input_ids'])\n",
        "train_mask0 = torch.tensor(tokens_train0['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "# for train set 1\n",
        "train_seq1, train_mask1 = torch.tensor(tokens_train1['input_ids']), torch.tensor(tokens_train1['attention_mask'])\n",
        "# for validation set 0\n",
        "val_seq0 = torch.tensor(tokens_val0['input_ids'])\n",
        "val_mask0 = torch.tensor(tokens_val0['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "# for validation set 1\n",
        "val_seq1, val_mask1 = torch.tensor(tokens_val1['input_ids']), torch.tensor(tokens_val1['attention_mask'])\n",
        "# for test set 0\n",
        "test_seq0 = torch.tensor(tokens_test0['input_ids'])\n",
        "test_mask0 = torch.tensor(tokens_test0['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())\n",
        "# for test set 1\n",
        "test_seq1, test_mask1 = torch.tensor(tokens_test1['input_ids']), torch.tensor(tokens_test1['attention_mask'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbQqi-_doG5i"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#define a batch size\n",
        "batch_size = 16\n",
        "# wrap tensors\n",
        "train_data0, train_data1 = TensorDataset(train_seq0, train_mask0, train_y), TensorDataset(train_seq1, train_mask1, train_y)\n",
        "# dataLoader for train set\n",
        "train_dataloader0 = DataLoader(train_data0, sampler=None, batch_size=batch_size)\n",
        "train_dataloader1 = DataLoader(train_data1, sampler=None, batch_size=batch_size)\n",
        "# wrap tensors\n",
        "val_data0, val_data1 = TensorDataset(val_seq0, val_mask0, val_y), TensorDataset(val_seq1, val_mask1, val_y)\n",
        "# sampler for sampling the data during training\n",
        "val_sampler0, val_sampler1 = SequentialSampler(val_data0), SequentialSampler(val_data1)\n",
        "# dataLoader for validation set\n",
        "val_dataloader0 = DataLoader(val_data0, sampler = val_sampler0, batch_size=batch_size)\n",
        "val_dataloader1 = DataLoader(val_data1, sampler = val_sampler1, batch_size=batch_size)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx7YJVxyuu1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7027272-ba5f-4a11-a684-fcb33bfd7a67"
      },
      "source": [
        "#convert class weights to tensor\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "print(class_wts)\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "epochs = 10"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.00421771 0.99581757]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cbqnecCu1i0"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  # iterate over batches\n",
        "  for step, batch in enumerate(train_dataloader0):\n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader0)))\n",
        "    for idx, bay in enumerate(train_dataloader1):\n",
        "      if idx == step :batch11 = bay\n",
        "    # push the batch to gpu\n",
        "    batch0 = [r.to(device) for r in batch]\n",
        "    batch1 = [r.to(device) for r in batch11]\n",
        "    sent_id0, mask0, labelss = batch0\n",
        "    sent_id1, mask1, labels = batch1\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "    # get model predictions for the current batch\n",
        "    loss, preds, _, _ = model(sent_id0, sent_id1, labels)\n",
        "    x,y = preds\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader0)\n",
        "  #returns the loss \n",
        "  return avg_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osSayFrku4MJ"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  print(\"\\nEvaluating...\")\n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  # iterate over batches\n",
        "  x1 = 0\n",
        "  for step,batch in enumerate(val_dataloader0):\n",
        "    for idx, bay in enumerate(train_dataloader1):\n",
        "      if idx == step :batch11 = bay\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader0)))\n",
        "    batch0 = [r.to(device) for r in batch]\n",
        "    batch1 = [r.to(device) for r in batch11]\n",
        "    sent_id0, mask0, labelss = batch0\n",
        "    sent_id1, mask1, labels = batch1\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      if len(sent_id0) == x1:\n",
        "        loss,preds,_,_ = model(sent_id0, sent_id1, labelss)\n",
        "        x,y = preds\n",
        "        # compute the loss between actual and predicted values\n",
        "        total_loss = total_loss + loss.item()\n",
        "        x1 = len(sent_id0)\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader0) \n",
        "  return avg_loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_kFmAR2vWPo",
        "outputId": "0689711c-7658-4e28-cedf-84c2f513af4e"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    #train model\n",
        "    train_loss = train()\n",
        "    #evaluate model\n",
        "    valid_loss = evaluate()\n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    625.\n",
            "  Batch   100  of    625.\n",
            "  Batch   150  of    625.\n",
            "  Batch   200  of    625.\n",
            "  Batch   250  of    625.\n",
            "  Batch   300  of    625.\n",
            "  Batch   350  of    625.\n",
            "  Batch   400  of    625.\n",
            "  Batch   450  of    625.\n",
            "  Batch   500  of    625.\n",
            "  Batch   550  of    625.\n",
            "  Batch   600  of    625.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     63.\n",
            "\n",
            "Training Loss: 0.202\n",
            "Validation Loss: 0.000\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    625.\n",
            "  Batch   100  of    625.\n",
            "  Batch   150  of    625.\n",
            "  Batch   200  of    625.\n",
            "  Batch   250  of    625.\n",
            "  Batch   300  of    625.\n",
            "  Batch   350  of    625.\n",
            "  Batch   400  of    625.\n",
            "  Batch   450  of    625.\n",
            "  Batch   500  of    625.\n",
            "  Batch   550  of    625.\n",
            "  Batch   600  of    625.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     63.\n",
            "\n",
            "Training Loss: 0.073\n",
            "Validation Loss: 0.000\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    625.\n",
            "  Batch   100  of    625.\n",
            "  Batch   150  of    625.\n",
            "  Batch   200  of    625.\n",
            "  Batch   250  of    625.\n",
            "  Batch   300  of    625.\n",
            "  Batch   350  of    625.\n",
            "  Batch   400  of    625.\n",
            "  Batch   450  of    625.\n",
            "  Batch   500  of    625.\n",
            "  Batch   550  of    625.\n",
            "  Batch   600  of    625.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     63.\n",
            "\n",
            "Training Loss: 0.038\n",
            "Validation Loss: 0.000\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    625.\n",
            "  Batch   100  of    625.\n",
            "  Batch   150  of    625.\n",
            "  Batch   200  of    625.\n",
            "  Batch   250  of    625.\n",
            "  Batch   300  of    625.\n",
            "  Batch   350  of    625.\n",
            "  Batch   400  of    625.\n",
            "  Batch   450  of    625.\n",
            "  Batch   500  of    625.\n",
            "  Batch   550  of    625.\n",
            "  Batch   600  of    625.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     63.\n",
            "\n",
            "Training Loss: 0.023\n",
            "Validation Loss: 0.000\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    625.\n",
            "  Batch   100  of    625.\n",
            "  Batch   150  of    625.\n",
            "  Batch   200  of    625.\n",
            "  Batch   250  of    625.\n",
            "  Batch   300  of    625.\n",
            "  Batch   350  of    625.\n",
            "  Batch   400  of    625.\n",
            "  Batch   450  of    625.\n",
            "  Batch   500  of    625.\n",
            "  Batch   550  of    625.\n",
            "  Batch   600  of    625.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     63.\n",
            "\n",
            "Training Loss: 0.015\n",
            "Validation Loss: 0.000\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    625.\n",
            "  Batch   100  of    625.\n",
            "  Batch   150  of    625.\n",
            "  Batch   200  of    625.\n",
            "  Batch   250  of    625.\n",
            "  Batch   300  of    625.\n",
            "  Batch   350  of    625.\n",
            "  Batch   400  of    625.\n",
            "  Batch   450  of    625.\n",
            "  Batch   500  of    625.\n",
            "  Batch   550  of    625.\n",
            "  Batch   600  of    625.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     63.\n",
            "\n",
            "Training Loss: 0.012\n",
            "Validation Loss: 0.000\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    625.\n",
            "  Batch   100  of    625.\n",
            "  Batch   150  of    625.\n",
            "  Batch   200  of    625.\n",
            "  Batch   250  of    625.\n",
            "  Batch   300  of    625.\n",
            "  Batch   350  of    625.\n",
            "  Batch   400  of    625.\n",
            "  Batch   450  of    625.\n",
            "  Batch   500  of    625.\n",
            "  Batch   550  of    625.\n",
            "  Batch   600  of    625.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     63.\n",
            "\n",
            "Training Loss: 0.009\n",
            "Validation Loss: 0.000\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    625.\n",
            "  Batch   100  of    625.\n",
            "  Batch   150  of    625.\n",
            "  Batch   200  of    625.\n",
            "  Batch   250  of    625.\n",
            "  Batch   300  of    625.\n",
            "  Batch   350  of    625.\n",
            "  Batch   400  of    625.\n",
            "  Batch   450  of    625.\n",
            "  Batch   500  of    625.\n",
            "  Batch   550  of    625.\n",
            "  Batch   600  of    625.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     63.\n",
            "\n",
            "Training Loss: 0.009\n",
            "Validation Loss: 0.000\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    625.\n",
            "  Batch   100  of    625.\n",
            "  Batch   150  of    625.\n",
            "  Batch   200  of    625.\n",
            "  Batch   250  of    625.\n",
            "  Batch   300  of    625.\n",
            "  Batch   350  of    625.\n",
            "  Batch   400  of    625.\n",
            "  Batch   450  of    625.\n",
            "  Batch   500  of    625.\n",
            "  Batch   550  of    625.\n",
            "  Batch   600  of    625.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     63.\n",
            "\n",
            "Training Loss: 0.009\n",
            "Validation Loss: 0.000\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    625.\n",
            "  Batch   100  of    625.\n",
            "  Batch   150  of    625.\n",
            "  Batch   200  of    625.\n",
            "  Batch   250  of    625.\n",
            "  Batch   300  of    625.\n",
            "  Batch   350  of    625.\n",
            "  Batch   400  of    625.\n",
            "  Batch   450  of    625.\n",
            "  Batch   500  of    625.\n",
            "  Batch   550  of    625.\n",
            "  Batch   600  of    625.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     63.\n",
            "\n",
            "Training Loss: 0.006\n",
            "Validation Loss: 0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeaimNTCaPXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fffbe30-4d3b-43b7-97b3-cd22bcc71a34"
      },
      "source": [
        "#load weights of best model\n",
        "model_name = 'model.pt'\n",
        "model.load_state_dict(torch.load(model_name))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwwEh_hJek7S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "99818b7a-c114-42d2-9e99-55723b001e77"
      },
      "source": [
        "with torch.no_grad():\n",
        "  preds,_,_ = model(test_seq0.to(device), test_seq1.to(device))\n",
        "  x,y = preds\n",
        "A = x.detach().cpu().numpy() - y.detach().cpu().numpy()\n",
        "z  = (A > 0).astype(int)\n",
        "pd.crosstab(test_y, z)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>450</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>62</td>\n",
              "      <td>430</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0    0    1\n",
              "row_0          \n",
              "0      450   58\n",
              "1       62  430"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLNnhZ8eroJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d34d12-15b7-46e6-ce08-230323e4077c"
      },
      "source": [
        "a=0\n",
        "for i in range(len(test_y)):\n",
        "  if test_y[i] == z[i]:a+=1\n",
        "print('Total Accuracy on Test Set:', float(a/len(test_y)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Accuracy on Test Set: 0.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q52qqbutNRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f39cfff5-5d88-47ea-f1a1-1bcf43fc5652"
      },
      "source": [
        "def check(l1,l2):\r\n",
        "  t1, t2 = tokenizer.tokenize(l1), tokenizer.tokenize(l2)\r\n",
        "  t11, t21 = torch.tensor([tokenizer.convert_tokens_to_ids(t1)]), torch.tensor([tokenizer.convert_tokens_to_ids(t2)])\r\n",
        "  preds,_,_ = model(t11.to(device), t21.to(device))\r\n",
        "  x,y = preds\r\n",
        "  A = x.detach().cpu().numpy() - y.detach().cpu().numpy()\r\n",
        "  z  = (A > 0).astype(int)\r\n",
        "  return z[0]\r\n",
        "a = \"whales are huge\"\r\n",
        "b = \"all whales are small\"\r\n",
        "print('returns the wrong answer:', check(a,b))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "returns the wrong answer: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WULVOsC2t-Mh"
      },
      "source": [
        "test_labi = pd.read_csv(\"subtaskA_gold_answers.csv\",header=None)\r\n",
        "test_labi = test_labi.drop([1], axis=1)\r\n",
        "test_labi['valid'] = z\r\n",
        "pd.DataFrame(test_labi).to_csv(\"subtaskA_answers.csv\", header=None, index = False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2QyUulEl7YK"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "jkIW0tjelyTN",
        "outputId": "bf9cc9f6-0b2d-4abe-9619-41a96591304c"
      },
      "source": [
        "epoochs = range(0,10)\r\n",
        "plt.plot(epoochs, train_losses, 'g', label='Training loss')\r\n",
        "plt.title('Training and Validation loss')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou0tAzsCmOWb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}